{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data.datasets import MNIST\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from itertools import product\n",
    "from sklearn import svm\n",
    "# argument parsing\n",
    "print (torch.__version__)\n",
    "batch_size=1\n",
    "test_batch_size=1\n",
    "kwargs={}\n",
    "train_loader=data_utils.DataLoader(MNIST(root='./data',train=True,process=False,transform=transforms.Compose([\n",
    "    transforms.Scale((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "])),batch_size=batch_size,shuffle=True,**kwargs)\n",
    "\n",
    "\n",
    "test_loader=data_utils.DataLoader(MNIST(root='./data',train=False,process=False,transform=transforms.Compose([\n",
    "    transforms.Scale((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "])),batch_size=test_batch_size,shuffle=True,**kwargs)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @ For demo use, only extracts the first 1000 samples\n",
    "# '''\n",
    "def create_numpy_dataset():\n",
    "    datasets = []\n",
    "    datalabel = []\n",
    "    for data in train_loader:\n",
    "        data_numpy = data[0].numpy()\n",
    "        data_numpy = np.squeeze(data_numpy)\n",
    "        datasets.append(data_numpy)\n",
    "        datalabel.append(data[1].numpy())\n",
    "\n",
    "    datasets = np.array(datasets)\n",
    "    datasets=np.expand_dims(datasets,axis=1)\n",
    "    print( 'Numpy dataset shape is {}'.format(datasets.shape))\n",
    "    return datasets[:60000],datalabel[:60000]\n",
    "\n",
    "def create_numpy_testdataset():\n",
    "    datasets = []\n",
    "    datalabel = []\n",
    "    for data in test_loader:\n",
    "        data_numpy = data[0].numpy()\n",
    "        data_numpy = np.squeeze(data_numpy)\n",
    "        datasets.append(data_numpy)\n",
    "        datalabel.append(data[1].numpy())\n",
    "\n",
    "    datasets = np.array(datasets)\n",
    "    datasets=np.expand_dims(datasets,axis=1)\n",
    "    print( 'Numpy dataset shape is {}'.format(datasets.shape))\n",
    "    return datasets[:9000],datalabel[:9000]\n",
    "\n",
    "\n",
    "# @ data: flatten patch data: (14*14*60000,1,2,2)\n",
    "# @ return: augmented anchors\n",
    "# '''\n",
    "def PCA_and_augment(data_in,number_important):\n",
    "    # data reshape\n",
    "    data=np.reshape(data_in,(data_in.shape[0],-1))\n",
    "    print( 'PCA_and_augment: {}'.format(data.shape))\n",
    "    # mean removal\n",
    "    mean = np.mean(data, axis=0)\n",
    "    datas_mean_remov = data - mean\n",
    "    print( 'PCA_and_augment meanremove shape: {}'.format(datas_mean_remov.shape))\n",
    "\n",
    "    # PCA, retain all components\n",
    "    pca=PCA(n_components = number_important)\n",
    "    pca.fit(datas_mean_remov)\n",
    "    comps=pca.components_\n",
    "\n",
    "    # augment, DC component doesn't\n",
    "    comps_aug=[vec*(-1) for vec in comps]\n",
    "    comps_complete=np.vstack((comps,comps_aug))\n",
    "    shapeComps = comps.shape\n",
    "    mean_kernel = np.ones(shapeComps[1])\n",
    "    mean_kernel = mean_kernel / np.sqrt(shapeComps[1])\n",
    "    comps_complete = np.vstack((comps_complete,mean_kernel))\n",
    "    print( 'PCA_and_augment comps_complete shape: {}'.format(comps_complete.shape))\n",
    "    return comps,comps_complete\n",
    "\n",
    "\n",
    "\n",
    "# '''\n",
    "# @ datasets: numpy data as input\n",
    "# @ depth: determine shape, initial: 0\n",
    "# '''\n",
    "\n",
    "def fit_pca_shape(datasets,depth):\n",
    "    factor=np.power(2,depth)\n",
    "    length=32/factor\n",
    "    print ('fit_pca_shape: length: {}'.format(length))\n",
    "    idx1=range(0,int(length),2)\n",
    "    idx2=[i+2 for i in idx1]\n",
    "    print ('fit_pca_shape: idx1: {}'.format(idx1))\n",
    "    data_lattice=[datasets[:,:,i:j,k:l] for ((i,j),(k,l)) in product(zip(idx1,idx2),zip(idx1,idx2))]\n",
    "    data_lattice=np.array(data_lattice)\n",
    "    print ('fit_pca_shape: data_lattice.shape: {}'.format(data_lattice.shape))\n",
    "\n",
    "    #shape reshape\n",
    "    data=np.reshape(data_lattice,(data_lattice.shape[0]*data_lattice.shape[1],data_lattice.shape[2],2,2))\n",
    "    print ('fit_pca_shape: reshape: {}'.format(data.shape))\n",
    "    return data\n",
    "\n",
    "\n",
    "# '''\n",
    "# @ Prepare shape changes. \n",
    "# @ return filters and datasets for convolution\n",
    "# @ aug_anchors: [7,4] -> [7,input_shape,2,2]\n",
    "# @ output_datas: [60000*num_patch*num_patch,channel,2,2]\n",
    "\n",
    "# '''\n",
    "def ret_filt_patches(aug_anchors,input_channels):\n",
    "    shape=int(aug_anchors.shape[1]/4)\n",
    "    num=aug_anchors.shape[0]\n",
    "    filt=np.reshape(aug_anchors,(num,shape,4))\n",
    "\n",
    "    # reshape to kernels, (7,shape,2,2)\n",
    "    filters=np.reshape(filt,(num,shape,2,2))\n",
    "\n",
    "    # reshape datasets, (60000*shape*shape,shape,28,28)\n",
    "    # datasets=np.expand_dims(dataset,axis=1)\n",
    "\n",
    "    return filters\n",
    "\n",
    "\n",
    "\n",
    "# '''\n",
    "# @ input: numpy kernel and data\n",
    "# @ output: conv+relu result\n",
    "# '''\n",
    "def conv_and_relu(filters,datasets,stride=2):\n",
    "    # torch data change\n",
    "    filters_t=torch.from_numpy(filters)\n",
    "    datasets_t=torch.from_numpy(datasets)\n",
    "\n",
    "    # Variables\n",
    "    filt=Variable(filters_t).type(torch.FloatTensor)\n",
    "    data=Variable(datasets_t).type(torch.FloatTensor)\n",
    "\n",
    "    # Convolution\n",
    "    output=F.conv2d(data,filt,stride=stride)\n",
    "\n",
    "    # Relu\n",
    "    relu_output=F.relu(output)\n",
    "\n",
    "    return relu_output,filt\n",
    "\n",
    "\n",
    "\n",
    "# @ One-stage Saak transform\n",
    "# @ input: datasets [60000,channel, size,size]\n",
    "# '''\n",
    "def one_stage_saak_trans(datasets=None,depth=0,number_important=3):\n",
    "\n",
    "\n",
    "    # load dataset, (60000,1,32,32)\n",
    "    # input_channel: 1->7\n",
    "    print ('one_stage_saak_trans: datasets.shape {}'.format(datasets.shape))\n",
    "    input_channels=datasets.shape[1]\n",
    "\n",
    "    # change data shape, (14*60000,4)\n",
    "    data_flatten=fit_pca_shape(datasets,depth)\n",
    "\n",
    "    # augmented components\n",
    "    comps,comps_complete=PCA_and_augment(data_flatten,number_important)\n",
    "    print ('one_stage_saak_trans: comps_complete: {}'.format(comps_complete.shape))\n",
    "    # print('one_saak_trans, non-aug kernel size:{}'.format(comps.shape))\n",
    "\n",
    "    # get filter and datas, (7,1,2,2) (60000,1,32,32)\n",
    "    filters=ret_filt_patches(comps_complete,input_channels)\n",
    "    print ('one_stage_saak_trans: filters: {}'.format(filters.shape))\n",
    "\n",
    "    # output (60000,7,14,14)\n",
    "    relu_output,filt=conv_and_relu(filters,datasets,stride=2)\n",
    "\n",
    "    data=relu_output.data.numpy()\n",
    "    print ('one_stage_saak_trans: output: {}'.format(data.shape))\n",
    "    return data,filt,relu_output\n",
    "\n",
    "\n",
    "\n",
    "# '''\n",
    "# @ Multi-stage Saak transform\n",
    "# '''\n",
    "def multi_stage_saak_trans():\n",
    "    filters = []\n",
    "    outputs = []\n",
    "    data,datalabel=create_numpy_dataset()\n",
    "    dataset=data\n",
    "    num=0\n",
    "    img_len=data.shape[-1]\n",
    "    while(img_len>=2):\n",
    "        num+=1\n",
    "        img_len/=2\n",
    "\n",
    "    stage_number = {0:3,1:4,2:7,3:6,4:8}\n",
    "    for i in range(num):\n",
    "        print ('{} stage of saak transform: '.format(i))\n",
    "        data,filt,output=one_stage_saak_trans(data,depth=i,number_important=stage_number[i])\n",
    "        filters.append(filt)\n",
    "        outputs.append(output)\n",
    "        print ('')\n",
    "\n",
    "\n",
    "    return dataset,filters,outputs,datalabel\n",
    "\n",
    "\n",
    "def multi_stage_saak_trans_test():\n",
    "    filters = []\n",
    "    outputs = []\n",
    "\n",
    "    data,datalabel=create_numpy_testdataset()\n",
    "    dataset=data\n",
    "    num=0\n",
    "    img_len=data.shape[-1]\n",
    "    while(img_len>=2):\n",
    "        num+=1\n",
    "        img_len/=2\n",
    "\n",
    "    stage_number = {0:3,1:4,2:7,3:6,4:8}\n",
    "    for i in range(num):\n",
    "        print ('{} stage of saak transform: '.format(i))\n",
    "        data,filt,output=one_stage_saak_trans(data,depth=i,number_important=stage_number[i])\n",
    "        filters.append(filt)\n",
    "        outputs.append(output)\n",
    "\n",
    "\n",
    "    return dataset,filters,outputs,datalabel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy dataset shape is (60000, 1, 32, 32)\n",
      "0 stage of saak transform: \n",
      "one_stage_saak_trans: datasets.shape (60000, 1, 32, 32)\n",
      "fit_pca_shape: length: 32.0\n",
      "fit_pca_shape: idx1: range(0, 32, 2)\n",
      "fit_pca_shape: data_lattice.shape: (256, 60000, 1, 2, 2)\n",
      "fit_pca_shape: reshape: (15360000, 1, 2, 2)\n",
      "PCA_and_augment: (15360000, 4)\n",
      "PCA_and_augment meanremove shape: (15360000, 4)\n",
      "PCA_and_augment comps_complete shape: (7, 4)\n",
      "one_stage_saak_trans: comps_complete: (7, 4)\n",
      "one_stage_saak_trans: filters: (7, 1, 2, 2)\n",
      "one_stage_saak_trans: output: (60000, 7, 16, 16)\n",
      "\n",
      "1 stage of saak transform: \n",
      "one_stage_saak_trans: datasets.shape (60000, 7, 16, 16)\n",
      "fit_pca_shape: length: 16.0\n",
      "fit_pca_shape: idx1: range(0, 16, 2)\n",
      "fit_pca_shape: data_lattice.shape: (64, 60000, 7, 2, 2)\n",
      "fit_pca_shape: reshape: (3840000, 7, 2, 2)\n",
      "PCA_and_augment: (3840000, 28)\n",
      "PCA_and_augment meanremove shape: (3840000, 28)\n",
      "PCA_and_augment comps_complete shape: (9, 28)\n",
      "one_stage_saak_trans: comps_complete: (9, 28)\n",
      "one_stage_saak_trans: filters: (9, 7, 2, 2)\n",
      "one_stage_saak_trans: output: (60000, 9, 8, 8)\n",
      "\n",
      "2 stage of saak transform: \n",
      "one_stage_saak_trans: datasets.shape (60000, 9, 8, 8)\n",
      "fit_pca_shape: length: 8.0\n",
      "fit_pca_shape: idx1: range(0, 8, 2)\n",
      "fit_pca_shape: data_lattice.shape: (16, 60000, 9, 2, 2)\n",
      "fit_pca_shape: reshape: (960000, 9, 2, 2)\n",
      "PCA_and_augment: (960000, 36)\n",
      "PCA_and_augment meanremove shape: (960000, 36)\n",
      "PCA_and_augment comps_complete shape: (15, 36)\n",
      "one_stage_saak_trans: comps_complete: (15, 36)\n",
      "one_stage_saak_trans: filters: (15, 9, 2, 2)\n",
      "one_stage_saak_trans: output: (60000, 15, 4, 4)\n",
      "\n",
      "3 stage of saak transform: \n",
      "one_stage_saak_trans: datasets.shape (60000, 15, 4, 4)\n",
      "fit_pca_shape: length: 4.0\n",
      "fit_pca_shape: idx1: range(0, 4, 2)\n",
      "fit_pca_shape: data_lattice.shape: (4, 60000, 15, 2, 2)\n",
      "fit_pca_shape: reshape: (240000, 15, 2, 2)\n",
      "PCA_and_augment: (240000, 60)\n",
      "PCA_and_augment meanremove shape: (240000, 60)\n",
      "PCA_and_augment comps_complete shape: (13, 60)\n",
      "one_stage_saak_trans: comps_complete: (13, 60)\n",
      "one_stage_saak_trans: filters: (13, 15, 2, 2)\n",
      "one_stage_saak_trans: output: (60000, 13, 2, 2)\n",
      "\n",
      "4 stage of saak transform: \n",
      "one_stage_saak_trans: datasets.shape (60000, 13, 2, 2)\n",
      "fit_pca_shape: length: 2.0\n",
      "fit_pca_shape: idx1: range(0, 2, 2)\n",
      "fit_pca_shape: data_lattice.shape: (1, 60000, 13, 2, 2)\n",
      "fit_pca_shape: reshape: (60000, 13, 2, 2)\n",
      "PCA_and_augment: (60000, 52)\n",
      "PCA_and_augment meanremove shape: (60000, 52)\n",
      "PCA_and_augment comps_complete shape: (17, 52)\n",
      "one_stage_saak_trans: comps_complete: (17, 52)\n",
      "one_stage_saak_trans: filters: (17, 13, 2, 2)\n",
      "one_stage_saak_trans: output: (60000, 17, 1, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    dataset, filters, outputs, datalabel = multi_stage_saak_trans()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now we can concatenate all the responses in every stage\n",
    "###### the total feature dimension is $16 \\times 16 \\times 7 + 8 \\times 8 \\times 9+4 \\times 4 \\times 15+2 \\times 2 \\times 13+17=2677$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imageAmount = 60000\n",
    "# features = np.zeros((imageAmount,2677))\n",
    "# for i in range(imageAmount):\n",
    "#     feature = []\n",
    "#     for j in range(len(outputs)):\n",
    "#         stagej = outputs[j].data.numpy()\n",
    "#         feature = np.concatenate([feature,stagej[i,:].flatten()])\n",
    "        \n",
    "#     features[i,:] = feature\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = feature_selection.SelectKBest(score_func = feature_selection.f_classif,k = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "datalabel = np.asarray(datalabel)\n",
    "datalabel = np.squeeze(datalabel)\n",
    "print(datalabel.shape)\n",
    "\n",
    "\n",
    "# #delete feature that has same value\n",
    "# features = features[:,~np.all(features[1:] == features[:-1],axis=0)]\n",
    "# print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# topFeatures = model.fit_transform(features,datalabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcaComNumber = 128\n",
    "# pca = PCA(n_components = pcaComNumber)\n",
    "# pcaTopFeatures = pca.fit_transform(topFeatures)\n",
    "# print(pcaTopFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 2677)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lychee\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [   0    1   14   15   16  224  240  255  256  257  270  271  272  287  479\n",
      "  480  495  496  509  510  511  512  513  526  527  528  544  720  736  752\n",
      "  753  754  766  767  768  769  770  771  772  773  774  775  776  777  778\n",
      "  779  780  781  782  783  784  785  786  787  788  789  790  791  792  793\n",
      "  794  795  796  797  798  799  800  801  802  803  804  805  806  807  808\n",
      "  809  810  811  812  813  814  815  816  817  818  819  820  821  822  823\n",
      "  824  825  826  827  828  829  830  831  832  833  834  835  836  837  838\n",
      "  839  840  841  842  843  844  845  846  847  848  849  850  851  852  853\n",
      "  854  855  856  857  858  859  860  861  862  863  864  865  866  867  868\n",
      "  869  870  871  872  873  874  875  876  877  878  879  880  881  882  883\n",
      "  884  885  886  887  888  889  890  891  892  893  894  895  896  897  898\n",
      "  899  900  901  902  903  904  905  906  907  908  909  910  911  912  913\n",
      "  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928\n",
      "  929  930  931  932  933  934  935  936  937  938  939  940  941  942  943\n",
      "  944  945  946  947  948  949  950  951  952  953  954  955  956  957  958\n",
      "  959  960  961  962  963  964  965  966  967  968  969  970  971  972  973\n",
      "  974  975  976  977  978  979  980  981  982  983  984  985  986  987  988\n",
      "  989  990  991  992  993  994  995  996  997  998  999 1000 1001 1002 1003\n",
      " 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018\n",
      " 1019 1020 1021 1022 1023 1024 1025 1026 1028 1038 1039 1040 1041 1056 1072\n",
      " 1088 1248 1264 1265 1279 1280 1281 1282 1283 1284 1285 1288 1289 1290 1291\n",
      " 1292 1293 1294 1295 1296 1310 1311 1327 1504 1520 1535 1536 1537 1550 1551\n",
      " 1552 1760 1776 1791 1919 1920 1927 1991 2048 2049 2050 2051 2052 2053 2054\n",
      " 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069\n",
      " 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084\n",
      " 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099\n",
      " 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2168 2232\n",
      " 2240 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493\n",
      " 2494 2495 2645 2647 2668] are constant.\n",
      "  UserWarning)\n",
      "C:\\Users\\lychee\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2677,)\n",
      "best f-score features size:(60000, 1000)\n",
      "after another round pca , feature size:(60000, 128)\n",
      "[False False False ..., False  True  True]\n",
      "(60000, 128)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_selection\n",
    "\n",
    "def feature_selection_pca(k_best_fscore, n_comps, image_amount,outputs,datalabel):\n",
    "    features = np.zeros((image_amount,2677))\n",
    "    for i in range(image_amount):\n",
    "        feature = []\n",
    "        for j in range(len(outputs)):\n",
    "            stagej = outputs[j].data.numpy()\n",
    "            feature = np.concatenate([feature,stagej[i,:].flatten()])   \n",
    "        features[i,:] = feature\n",
    "    \n",
    "    model = feature_selection.SelectKBest(score_func = feature_selection.f_classif,k = k_best_fscore)\n",
    "    datalabel = np.asarray(datalabel)\n",
    "    datalabel = np.squeeze(datalabel)\n",
    "    print(datalabel.shape)\n",
    "\n",
    "    print(features.shape)\n",
    "    topFeatures = model.fit_transform(features,datalabel)\n",
    "    indexs = model.get_support()\n",
    "    print(indexs.shape)\n",
    "    print(\"best f-score features size:{}\".format(topFeatures.shape))\n",
    "    pcaComNumber = n_comps\n",
    "    pca = PCA(n_components = pcaComNumber)\n",
    "    pcaTopFeatures = pca.fit_transform(topFeatures)\n",
    "    print(\"after another round pca , feature size:{}\".format(pcaTopFeatures.shape))\n",
    "    return pcaTopFeatures,indexs,pca\n",
    "\n",
    "\n",
    "def feature_selection_pca_for_test(k_best_fscore, n_comps, image_amount,outputs,learnedIndex,pca):\n",
    "    features = np.zeros((image_amount,2677))\n",
    "    for i in range(image_amount):\n",
    "        feature = []\n",
    "        for j in range(len(outputs)):\n",
    "            stagej = outputs[j].data.numpy()\n",
    "            feature = np.concatenate([feature,stagej[i,:].flatten()])   \n",
    "        features[i,:] = feature\n",
    "        \n",
    "    topFeatures = features[:,learnedIndex]\n",
    "    print(\"best f-score features size:{}\".format(topFeatures.shape))\n",
    "    pcaTopFeatures = pca.fit_transform(topFeatures)\n",
    "    print(\"after another round pca , feature size:{}\".format(pcaTopFeatures.shape))\n",
    "    return pcaTopFeatures\n",
    "    \n",
    "    \n",
    "    \n",
    "pcaTopFeatures,indexs,pca = feature_selection_pca(1000,128,60000,outputs,datalabel)\n",
    "print(indexs)\n",
    "print(pcaTopFeatures.shape)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### till now the 128 features for the 60000 training images is complete , next is to train the svm classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now perform saak on the 10000 testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm training start\n",
      "svm training finished\n"
     ]
    }
   ],
   "source": [
    "print(\"svm training start\")\n",
    "clf = svm.SVC()\n",
    "clf.fit(pcaTopFeatures,datalabel)\n",
    "print(\"svm training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy dataset shape is (10000, 1, 32, 32)\n",
      "0 stage of saak transform: \n",
      "one_stage_saak_trans: datasets.shape (9000, 1, 32, 32)\n",
      "fit_pca_shape: length: 32.0\n",
      "fit_pca_shape: idx1: range(0, 32, 2)\n",
      "fit_pca_shape: data_lattice.shape: (256, 9000, 1, 2, 2)\n",
      "fit_pca_shape: reshape: (2304000, 1, 2, 2)\n",
      "PCA_and_augment: (2304000, 4)\n",
      "PCA_and_augment meanremove shape: (2304000, 4)\n",
      "PCA_and_augment comps_complete shape: (7, 4)\n",
      "one_stage_saak_trans: comps_complete: (7, 4)\n",
      "one_stage_saak_trans: filters: (7, 1, 2, 2)\n",
      "one_stage_saak_trans: output: (9000, 7, 16, 16)\n",
      "1 stage of saak transform: \n",
      "one_stage_saak_trans: datasets.shape (9000, 7, 16, 16)\n",
      "fit_pca_shape: length: 16.0\n",
      "fit_pca_shape: idx1: range(0, 16, 2)\n",
      "fit_pca_shape: data_lattice.shape: (64, 9000, 7, 2, 2)\n",
      "fit_pca_shape: reshape: (576000, 7, 2, 2)\n",
      "PCA_and_augment: (576000, 28)\n",
      "PCA_and_augment meanremove shape: (576000, 28)\n",
      "PCA_and_augment comps_complete shape: (9, 28)\n",
      "one_stage_saak_trans: comps_complete: (9, 28)\n",
      "one_stage_saak_trans: filters: (9, 7, 2, 2)\n",
      "one_stage_saak_trans: output: (9000, 9, 8, 8)\n",
      "2 stage of saak transform: \n",
      "one_stage_saak_trans: datasets.shape (9000, 9, 8, 8)\n",
      "fit_pca_shape: length: 8.0\n",
      "fit_pca_shape: idx1: range(0, 8, 2)\n",
      "fit_pca_shape: data_lattice.shape: (16, 9000, 9, 2, 2)\n",
      "fit_pca_shape: reshape: (144000, 9, 2, 2)\n",
      "PCA_and_augment: (144000, 36)\n",
      "PCA_and_augment meanremove shape: (144000, 36)\n",
      "PCA_and_augment comps_complete shape: (15, 36)\n",
      "one_stage_saak_trans: comps_complete: (15, 36)\n",
      "one_stage_saak_trans: filters: (15, 9, 2, 2)\n",
      "one_stage_saak_trans: output: (9000, 15, 4, 4)\n",
      "3 stage of saak transform: \n",
      "one_stage_saak_trans: datasets.shape (9000, 15, 4, 4)\n",
      "fit_pca_shape: length: 4.0\n",
      "fit_pca_shape: idx1: range(0, 4, 2)\n",
      "fit_pca_shape: data_lattice.shape: (4, 9000, 15, 2, 2)\n",
      "fit_pca_shape: reshape: (36000, 15, 2, 2)\n",
      "PCA_and_augment: (36000, 60)\n",
      "PCA_and_augment meanremove shape: (36000, 60)\n",
      "PCA_and_augment comps_complete shape: (13, 60)\n",
      "one_stage_saak_trans: comps_complete: (13, 60)\n",
      "one_stage_saak_trans: filters: (13, 15, 2, 2)\n",
      "one_stage_saak_trans: output: (9000, 13, 2, 2)\n",
      "4 stage of saak transform: \n",
      "one_stage_saak_trans: datasets.shape (9000, 13, 2, 2)\n",
      "fit_pca_shape: length: 2.0\n",
      "fit_pca_shape: idx1: range(0, 2, 2)\n",
      "fit_pca_shape: data_lattice.shape: (1, 9000, 13, 2, 2)\n",
      "fit_pca_shape: reshape: (9000, 13, 2, 2)\n",
      "PCA_and_augment: (9000, 52)\n",
      "PCA_and_augment meanremove shape: (9000, 52)\n",
      "PCA_and_augment comps_complete shape: (17, 52)\n",
      "one_stage_saak_trans: comps_complete: (17, 52)\n",
      "one_stage_saak_trans: filters: (17, 13, 2, 2)\n",
      "one_stage_saak_trans: output: (9000, 17, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "datatestSet, filters, outputsTest, datalabeltest = multi_stage_saak_trans_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f-score features size:(9000, 1000)\n",
      "after another round pca , feature size:(9000, 128)\n"
     ]
    }
   ],
   "source": [
    "pcaTopFeaturesTest = feature_selection_pca_for_test(1000,128,9000,outputsTest,indexs,pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.397\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "for i in range(9000):\n",
    "    if(clf.predict([pcaTopFeaturesTest[i]]) != datalabeltest[i]):\n",
    "        error = error +1\n",
    "print(error/9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
